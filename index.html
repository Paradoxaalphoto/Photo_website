<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>YorN — Alpha (Detection Fix)</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
  <style>.mono{font-family:ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace}</style>
</head>
<body class="bg-gray-50 text-gray-900">
  <div class="max-w-4xl mx-auto p-4 space-y-4">
    <header>
      <h1 class="text-2xl font-bold">YorN — Alpha</h1>
      <p class="text-sm text-gray-600">Mobile detection hardening. Try your photo or a sample image.</p>
    </header>

    <section class="grid md:grid-cols-[1fr_auto_auto] gap-3 items-end bg-white p-4 rounded-xl shadow">
      <input id="fileInput" type="file" accept="image/*" class="border p-2 rounded w-full" />
      <button id="sampleBtn" class="px-3 py-2 border rounded">Load Sample Image</button>
      <button id="analyzeBtn" class="px-4 py-2 bg-blue-600 text-white rounded disabled:opacity-50" disabled>Analyze</button>
      <div class="col-span-full w-full bg-gray-200 h-2 rounded overflow-hidden">
        <div id="progressBar" class="bg-blue-600 h-2 w-0"></div>
      </div>
      <div id="progressText" class="col-span-full text-xs text-gray-600"></div>
    </section>

    <section class="grid md:grid-cols-2 gap-4">
      <div class="bg-white p-4 rounded-xl shadow">
        <h2 class="text-sm font-semibold mb-2">Preview</h2>
        <img id="thumb" class="hidden w-full max-h-96 object-contain rounded border bg-white" alt="Selected preview" />
        <canvas id="overlay" class="hidden border rounded bg-white mt-3"></canvas>
      </div>
      <div class="bg-white p-4 rounded-xl shadow">
        <h2 class="text-sm font-semibold mb-2">Diagnostics</h2>
        <div id="diag" class="text-xs mono whitespace-pre-wrap bg-gray-50 border rounded p-2 max-h-80 overflow-auto">No diagnostics yet.</div>
        <div id="errorBox" class="text-sm text-red-700 bg-red-50 border border-red-200 rounded p-2 mt-2 hidden"></div>
      </div>
    </section>
  </div>

  <script>
    const CDNS = [
      "https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights",
      "https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js@0.22.2/weights"
    ];
    let WEIGHTS = null, modelsLoaded = false, imageBlobUrl = null;

    const $ = id => document.getElementById(id);
    const fileInput = $("fileInput"), sampleBtn = $("sampleBtn"), analyzeBtn = $("analyzeBtn");
    const thumb = $("thumb"), overlay = $("overlay");
    const progressBar = $("progressBar"), progressText = $("progressText");
    const diag = $("diag"), errorBox = $("errorBox");

    function setProgress(p,t){ progressBar.style.width=(p||0)+'%'; progressText.textContent=t||''; }
    function logDiag(obj){ const now=new Date().toISOString(); const txt=typeof obj==='string'?obj:JSON.stringify(obj,null,2); diag.textContent=`[${now}]\n${txt}\n\n`+diag.textContent; }
    function setError(msg){ errorBox.textContent=msg; errorBox.classList.remove("hidden"); logDiag({error:msg}); }
    function clearError(){ errorBox.textContent=""; errorBox.classList.add("hidden"); }

    fileInput.addEventListener("change", ()=>{
      if(!fileInput.files.length){ analyzeBtn.disabled=true; return; }
      const f=fileInput.files[0]; imageBlobUrl=URL.createObjectURL(f);
      thumb.src=imageBlobUrl; thumb.classList.remove("hidden"); analyzeBtn.disabled=false;
      overlay.classList.add("hidden"); clearError(); setProgress(0,"");
      logDiag({ fileName:f.name, type:f.type, size_bytes:f.size });
    });

    sampleBtn.addEventListener("click", ()=>{
      // Public domain face sample (Wikimedia)
      imageBlobUrl = "https://upload.wikimedia.org/wikipedia/commons/3/3f/Fronalpstock_big.jpg"; // contains faces in background; not ideal
      // Better: a single clear portrait (Creative Commons); fallback to data-URL if blocked.
      thumb.src = imageBlobUrl;
      thumb.classList.remove("hidden");
      analyzeBtn.disabled = false;
      overlay.classList.add("hidden"); clearError(); setProgress(0,"");
      logDiag({ sampleImage: imageBlobUrl });
    });

    async function pickWeightsCdn(){
      for(const b of CDNS){
        try{ const res=await fetch(b+"/face_landmark_68_model-weights_manifest.json",{method:"HEAD"});
          if(res.ok){ WEIGHTS=b; logDiag({ usingWeightsFrom:b }); return; }
          logDiag({ cdnAttempt:b, status:res.status });
        }catch(e){ logDiag({ cdnAttempt:b, error:e && e.message }); }
      }
      throw new Error("No working CDN for weights");
    }

    async function ensureModels(){
      if(modelsLoaded) return;
      setProgress(6,"Selecting CDN…"); if(!WEIGHTS) await pickWeightsCdn();
      setProgress(10,"Setting backend…");
      try{
        await tf.setBackend('webgl');
        await tf.ready();
      }catch(e){
        logDiag({ webglError: e && e.message });
        await tf.setBackend('cpu');
        await tf.ready();
      }
      setProgress(22,"Loading TinyFaceDetector…"); await faceapi.nets.tinyFaceDetector.loadFromUri(WEIGHTS);
      setProgress(40,"Loading Landmarks…"); await faceapi.nets.faceLandmark68Net.loadFromUri(WEIGHTS);
      modelsLoaded=true; setProgress(50,"Models ready."); logDiag({detector:"TinyFaceDetector", backend: tf.getBackend(), modelsLoaded});
    }

    function downscale(img, maxDim=1024){
      const scale = Math.min(1, maxDim/Math.max(img.width, img.height));
      const canvas = document.createElement('canvas');
      canvas.width = Math.round(img.width*scale);
      canvas.height = Math.round(img.height*scale);
      const ctx = canvas.getContext('2d');
      ctx.drawImage(img,0,0,canvas.width,canvas.height);
      const out = new Image();
      out.src = canvas.toDataURL('image/jpeg', 0.92);
      out.width = canvas.width; out.height = canvas.height;
      return new Promise(res=> out.onload=()=> res(out));
    }

    function preprocessVariants(img){
      return [
        { rotate:0, filter:"none", label:"rot0 base" },
        { rotate:90, filter:"none", label:"rot90" },
        { rotate:270, filter:"none", label:"rot270" },
        { rotate:0, filter:"brightness(1.15)", label:"+bright15" },
        { rotate:0, filter:"contrast(1.2)", label:"+contrast20" },
        { rotate:0, filter:"grayscale(0.3)", label:"gray30" }
      ];
    }

    function applyVariant(img, variant){
      const canvas = document.createElement('canvas');
      const rotate = variant.rotate || 0;
      const w = (rotate % 180 === 0) ? img.width : img.height;
      const h = (rotate % 180 === 0) ? img.height : img.width;
      canvas.width = w; canvas.height = h;
      const ctx = canvas.getContext('2d');
      ctx.translate(w/2, h/2);
      ctx.rotate(rotate * Math.PI/180);
      ctx.filter = variant.filter || "none";
      ctx.drawImage(img, -img.width/2, -img.height/2, img.width, img.height);
      const out = new Image();
      out.src = canvas.toDataURL('image/jpeg', 0.92);
      out.width = w; out.height = h;
      return new Promise(res=> out.onload=()=> res(out));
    }

    function detectWithTimeout(img, opts, ms=6000){
      return Promise.race([
        faceapi.detectSingleFace(img, opts).withFaceLandmarks(),
        new Promise((_, rej)=> setTimeout(()=> rej(new Error("Detection timeout")), ms))
      ]);
    }

    function drawDetection(img, det){
      const canvas = document.getElementById('overlay');
      canvas.width = img.width; canvas.height = img.height;
      const ctx = canvas.getContext('2d');
      ctx.clearRect(0,0,canvas.width,canvas.height);
      ctx.drawImage(img,0,0,img.width,img.height);
      const r = faceapi.resizeResults(det, { width: img.width, height: img.height });
      new faceapi.draw.DrawBox(r.detection.box, { label: `score ${r.detection.score.toFixed(2)}` }).draw(canvas);
      faceapi.draw.drawFaceLandmarks(canvas, r);
      canvas.classList.remove('hidden');
    }

    async function analyze(){
      try{
        clearError(); setProgress(5,"Starting…"); await ensureModels();
        if(!imageBlobUrl){ setProgress(0,"Choose a photo first."); return; }
        setProgress(58,"Loading image…");
        const raw = new Image();
        raw.onload = async ()=>{
          const scaled = await downscale(raw, 1024);
          const variants = preprocessVariants(scaled);
          const sizes = [320, 224, 160]; // mobile-friendly
          let found=null, finalImg=null, used=null;

          for(const v of variants){
            const img = await applyVariant(scaled, v);
            for(const s of sizes){
              try{
                setProgress(70, `Detecting (${v.label}, ${s})…`);
                const opts = new faceapi.TinyFaceDetectorOptions({ inputSize: s, scoreThreshold: 0.3 });
                const det = await detectWithTimeout(img, opts, 6000);
                if(det && det.detection){
                  found = det; finalImg = img; used = { variant: v.label, inputSize: s, score: det.detection.score };
                  break;
                }
              }catch(e){
                logDiag({ attempt:{variant:v.label, inputSize:s}, error:e && e.message });
              }
              await tf.nextFrame();
            }
            if(found) break;
          }

          if(!found){ setProgress(0,""); setError("Still couldn’t detect a face. Try a front-facing photo with your face larger in frame."); return; }

          drawDetection(finalImg, found);
          logDiag({ success:true, used });
          setProgress(100,"Done.");
          setTimeout(()=> setProgress(0,""), 1500);
        };
        raw.src = imageBlobUrl;
      }catch(e){
        setError("Error during analysis: " + (e && e.message));
        setProgress(0,"");
      }
    }

    document.getElementById('analyzeBtn').addEventListener('click', analyze);
  </script>
</body>
</html>
