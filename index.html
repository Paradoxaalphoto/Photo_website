<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>YorN Alpha</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
<style>
  body { font-family: sans-serif; padding: 10px; }
  #progressContainer { height: 20px; background: #eee; border-radius: 5px; overflow: hidden; margin: 10px 0; }
  #progressBar { height: 100%; width: 0%; background: #4caf50; transition: width 0.3s; }
  #overlay { position: relative; display: block; margin-top: 10px; }
  #diagnostics { font-size: 0.8em; white-space: pre-wrap; background: #f5f5f5; padding: 5px; border: 1px solid #ccc; margin-top: 10px; }
  #thumb { max-width: 100%; margin-top: 10px; }
</style>
</head>
<body>

<h2>YorN Alpha – Face Analysis</h2>

<input type="file" id="fileInput" accept="image/*"><br><br>
<div id="progressContainer"><div id="progressBar"></div></div>
<p id="progressText"></p>

<button id="analyzeBtn" disabled>Analyze</button>
<canvas id="overlay" class="hidden"></canvas>
<img id="thumb" class="hidden" alt="Preview">

<h3>Diagnostics</h3>
<div id="diagnostics"></div>

<script>
const fileInput = document.getElementById("fileInput");
const analyzeBtn = document.getElementById("analyzeBtn");
const overlay = document.getElementById("overlay");
const progressBar = document.getElementById("progressBar");
const progressText = document.getElementById("progressText");
const diagnostics = document.getElementById("diagnostics");
const thumb = document.getElementById("thumb");

let preparePromise = null;
let preparedImage = null;
let WEIGHTS = null;

function setProgress(pct, text) {
  progressBar.style.width = pct + "%";
  progressText.textContent = text || "";
}
function logDiag(obj) {
  diagnostics.textContent += JSON.stringify(obj) + "\n";
}
function clearDiag() { diagnostics.textContent = ""; }
function setError(msg) {
  logDiag({ error: msg });
  progressText.textContent = msg;
}

async function fetchJsonNoStore(url) {
  const res = await fetch(url, { method: "GET", cache: "no-store" });
  if (!res.ok) throw new Error(`status ${res.status}`);
  return res.json();
}
async function verifyManifestAndShards(base, manifestName) {
  const manifestUrl = `${base}/${manifestName}`;
  const manifest = await fetchJsonNoStore(manifestUrl);
  const shards = (manifest.weights || [])
    .flatMap(w => w.paths || [])
    .map(p => `${base}/${p}`);
  for (const shardUrl of shards) {
    const r = await fetch(shardUrl, { method: "GET", cache: "no-store" });
    if (!r.ok) throw new Error(`shard 404: ${shardUrl}`);
  }
}
async function verifyWeightsBase(base) {
  await verifyManifestAndShards(base, "tiny_face_detector_model-weights_manifest.json");
  await verifyManifestAndShards(base, "face_landmark_68_model-weights_manifest.json");
  return true;
}
async function pickWeightsCdn() {
  const CDN_BASES = [
    "https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights",
    "https://unpkg.com/face-api.js@0.22.2/weights",
    "https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js@0.22.2/weights",
    "https://rawcdn.githack.com/justadudewhohacks/face-api.js/0.22.2/weights",
    "https://cdn.jsdelivr.net/gh/vladmandic/face-api/model"
  ];
  for (const base of CDN_BASES) {
    try {
      await verifyWeightsBase(base);
      WEIGHTS = base.replace(/\/$/, "");
      logDiag({ usingWeightsFrom: WEIGHTS, verified: true });
      return;
    } catch (e) {
      logDiag({ cdnAttempt: base, fail: e.message });
    }
  }
  throw new Error("No working CDN with complete manifests + shards");
}

async function decodeAndPrepare(file, targetMax = 1024) {
  if (file.size > 15 * 1024 * 1024) {
    throw new Error("Photo is too large. Please pick one under 15MB.");
  }
  const opts = { imageOrientation: "from-image" };
  let bmp = await createImageBitmap(file, opts).catch(async () => {
    const url = URL.createObjectURL(file);
    const img = new Image();
    await new Promise((res, rej) => { img.onload = res; img.onerror = rej; img.src = url; });
    await img.decode?.().catch(()=>{});
    const c = document.createElement("canvas");
    c.width = img.naturalWidth; c.height = img.naturalHeight;
    c.getContext("2d").drawImage(img, 0, 0);
    URL.revokeObjectURL(url);
    bmp = await createImageBitmap(c);
  });
  const maxSide = Math.max(bmp.width, bmp.height);
  const scale = Math.min(1, targetMax / maxSide);
  const w = Math.round(bmp.width * scale);
  const h = Math.round(bmp.height * scale);
  let canvas;
  try { canvas = new OffscreenCanvas(w, h); } catch { canvas = document.createElement("canvas"); canvas.width = w; canvas.height = h; }
  const ctx = canvas.getContext("2d");
  ctx.imageSmoothingEnabled = true;
  ctx.imageSmoothingQuality = "high";
  ctx.drawImage(bmp, 0, 0, w, h);
  return { canvas, ctx, width: w, height: h };
}

fileInput.addEventListener("change", async () => {
  clearDiag();
  if (!fileInput.files.length) { analyzeBtn.disabled = true; return; }
  const f = fileInput.files[0];
  thumb.src = URL.createObjectURL(f);
  thumb.classList.remove("hidden");
  logDiag({ fileName: f.name, type: f.type, size_bytes: f.size });
  analyzeBtn.disabled = true;
  setProgress(10, "Preparing photo…");
  try {
    preparePromise = decodeAndPrepare(f, 1024);
    preparedImage = await preparePromise;
    analyzeBtn.disabled = false;
    setProgress(15, "Photo ready.");
  } catch (e) {
    setError("Could not prepare photo: " + e.message);
    analyzeBtn.disabled = true;
  }
});

async function detectWithTimeout(promise, ms) {
  let timeoutId;
  const timeout = new Promise((_, rej) => {
    timeoutId = setTimeout(() => rej(new Error("Detection timeout")), ms);
  });
  return Promise.race([promise, timeout]).finally(() => clearTimeout(timeoutId));
}

analyzeBtn.addEventListener("click", async () => {
  if (!preparedImage && preparePromise) preparedImage = await preparePromise;
  if (!preparedImage) { setError("No prepared photo."); return; }
  if (!WEIGHTS) await pickWeightsCdn();
  setProgress(20, "Loading models…");
  await faceapi.nets.tinyFaceDetector.loadFromUri(WEIGHTS);
  await faceapi.nets.faceLandmark68Net.loadFromUri(WEIGHTS);
  setProgress(40, "Models loaded.");
  let imgBitmap;
  if (preparedImage.canvas instanceof OffscreenCanvas) {
    imgBitmap = await preparedImage.canvas.convertToBlob({ type: "image/jpeg", quality: 0.92 })
      .then(blob => createImageBitmap(blob));
  } else {
    imgBitmap = await createImageBitmap(preparedImage.canvas);
  }
  overlay.width = preparedImage.width;
  overlay.height = preparedImage.height;
  const sizes = [192, 160, 128];
  let det = null, used = null;
  for (const s of sizes) {
    try {
      setProgress(70, `Detecting (size ${s})…`);
      const opts = new faceapi.TinyFaceDetectorOptions({ inputSize: s, scoreThreshold: 0.3 });
      det = await detectWithTimeout(faceapi.detectSingleFace(imgBitmap, opts).withFaceLandmarks(), 15000);
      used = s;
      if (det) break;
    } catch (e) {
      logDiag({ attemptFail: { size: s, msg: e.message } });
    }
  }
  if (!det) { setError("Timeout or no face."); return; }
  const ctx = overlay.getContext("2d");
  ctx.clearRect(0,0,overlay.width,overlay.height);
  ctx.drawImage(imgBitmap, 0, 0, preparedImage.width, preparedImage.height);
  const r = faceapi.resizeResults(det, { width: preparedImage.width, height: preparedImage.height });
  new faceapi.draw.DrawBox(r.detection.box, { label: `score ${r.detection.score.toFixed(2)} • ${used}` }).draw(overlay);
  faceapi.draw.drawFaceLandmarks(overlay, r);
  overlay.classList.remove("hidden");
  setProgress(100, "Done.");
});
</script>

</body>
</html>